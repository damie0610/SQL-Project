# Final Project: Transforming and Analyzing Data with SQL

## Project/Goals
This project is based on an ```ecommerce database``` which provides data on products offered and orders, customer usage of the ecommerce site - time of visit, location from which the site was accessed, length of time on site and other details regarding the visit and online transactions. Five ```.csv``` (5) tables are provided in this database.

- all_sessions
- analytics
- products
- sales_by_sku
- sales_report

My goal was to explore the attributes of each entity of the database, establish the links between each entity and analyze the relationships between attributes to provide insights regarding the ecommerce patterns/trends.


## Process

### Data Importation & Exploration:
 
Examining and understanding the characteristics and contents within the dataset, the datatypes, issues and anomalies - to determine appropriate paths for importing the data, identify features that are connected and ways to approach data cleaning and analysis.

### Data Cleaning & Transformation:
The raw data was cleaned and *wrangled* into a format suitable for analysis. The process involved addressing duplicates and missing values, data type conversions, insertion of constraints and data standardization among other iterative processes throughout the analysis.

### Data Querying & Quality Assurance:
Usinq SQL queries, information regarding patterns and insights from the ecommerce database was retrieved - using functions, aggregations, joins, unions and other SQL syntax . An iterative QA process was included to ensure validity of queries from data extraction to analysis.


## Results
(fill in what you discovered this data could tell you and how you used the data to answer those questions)

## Challenges 

### Insufficient Context: 
The database is provided with scanty context around the origin of the data which makes for a lot of assumption in the data transformation and analysis - hardly ideal for real-world data handling.

### High Volume of Duplicates: 
One of the datasets had duplicates had an inordinate amount of duplicates which caused constant crashes of the server. 

### Identifying Connections & Creating Primary Keys

## Future Goals
(what would you do if you had more time?)
